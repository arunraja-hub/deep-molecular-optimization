[I 2023-08-14 15:34:06,530] A new study created in memory with name: transformer-original-source2target-optuna-study
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:30: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:31: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:31: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:31: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:31: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:31: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:31: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:33: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:34: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:34: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:34: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:34: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:34: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:34: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:34: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:36: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:36: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:36: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:36: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:36: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:36: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:36: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:36: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:36: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:37: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:37: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:37: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:37: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:37: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:37: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:37: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:37: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:37: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:39: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:41: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:43: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py:59: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  nn.init.xavier_uniform(p)
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:46: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:49: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:51: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:53: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:54: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:55: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:57: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:58: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:35:59: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5}
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:00: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:01: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004}
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:02: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004}
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:04: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:05: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:05: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5}
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:00: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:03: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1}
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:06: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004}
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:06: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:07: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:07: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.train +214: INFO     Training start
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
15:36:08: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1}
  0%|          | 0/2513 [00:00<?, ?it/s]
  0%|          | 0/2513 [00:00<?, ?it/s][A

  0%|          | 0/2513 [00:00<?, ?it/s][A[A


  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A



  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A




  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A





  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A






  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A







  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A








  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A









  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start
15:36:09: transformer_trainer.train +214: INFO     Training start











  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A











  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A












  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A













  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A














  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:09: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:10: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5}
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:11: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start

















  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start
15:36:12: transformer_trainer.train +214: INFO     Training start


















  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:12: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start
15:36:13: transformer_trainer.train +214: INFO     Training start



















  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start




















  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start
15:36:14: transformer_trainer.train +214: INFO     Training start





















  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:15: transformer_trainer.get_model +47: INFO     Optuna current params:{'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004}
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start






















  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start























  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
























  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start

























  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start
15:36:16: transformer_trainer.train +214: INFO     Training start


























  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start



























  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start
15:36:17: transformer_trainer.train +214: INFO     Training start




























  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start





























  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start






























  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:18: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:18: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start

































  0%|          | 0/2513 [00:00<?, ?it/s]15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start


































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1



































  0%|          | 0/2513 [00:00<?, ?it/s]15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start




































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start





































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start






































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start







































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start








































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start









































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A














  0%|          | 1/2513 [00:11<7:45:07, 11.11s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start










































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1


  0%|          | 1/2513 [00:12<8:32:45, 12.25s/it][A[A15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start
15:36:19: transformer_trainer.train +214: INFO     Training start











































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1












































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1













































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start
15:36:20: transformer_trainer.train +214: INFO     Training start














































  0%|          | 0/2513 [00:00<?, ?it/s]15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:20: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start















































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
















































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start
15:36:21: transformer_trainer.train +214: INFO     Training start

















































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start


















































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start
15:36:22: transformer_trainer.train +214: INFO     Training start





















  0%|          | 1/2513 [00:08<5:33:14,  7.96s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:22: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1



















































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A










  0%|          | 1/2513 [00:13<9:34:45, 13.73s/it][A[A[A[A[A[A[A[A[A[A[A15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start





  0%|          | 1/2513 [00:14<10:09:26, 14.56s/it][A[A[A[A[A



  0%|          | 1/2513 [00:14<10:02:24, 14.39s/it][A[A[A[A



















































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


  0%|          | 1/2513 [00:14<10:15:21, 14.70s/it][A[A[A15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +212: INFO     Starting EPOCH #1
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start
15:36:23: transformer_trainer.train +214: INFO     Training start





















































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















  0%|          | 1/2513 [00:08<6:02:27,  8.66s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















































  0%|          | 0/2513 [00:00<?, ?it/s][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























  0%|          | 1/2513 [00:07<4:56:00,  7.07s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:15<10:49:12, 15.51s/it]







  0%|          | 1/2513 [00:15<10:48:06, 15.48s/it][A[A[A[A[A[A[A[A

















  0%|          | 1/2513 [00:11<7:54:16, 11.33s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























  0%|          | 1/2513 [00:08<5:35:27,  8.01s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























  0%|          | 1/2513 [00:07<5:23:44,  7.73s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























  0%|          | 1/2513 [00:08<5:43:23,  8.20s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















  0%|          | 1/2513 [00:08<5:51:55,  8.41s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























  0%|          | 1/2513 [00:06<4:24:32,  6.32s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A














  0%|          | 2/2513 [00:15<6:21:32,  9.12s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A











  0%|          | 1/2513 [00:15<11:03:00, 15.84s/it][A[A[A[A[A[A[A[A[A[A[A[A

  0%|          | 2/2513 [00:17<7:07:02, 10.20s/it][A[A



























  0%|          | 1/2513 [00:08<5:47:07,  8.29s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


  0%|          | 2/2513 [00:17<7:50:48, 11.25s/it] [A[A[A





















































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A














































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 0/2513 [00:05<?, ?it/s]





















































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 0/2513 [00:03<?, ?it/s]
  0%|          | 0/2513 [00:03<?, ?it/s]

















































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 0/2513 [00:04<?, ?it/s]
Exception ignored in: <function tqdm.__del__ at 0x7fdcb1146f80>
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/tqdm/std.py", line 1039, in __del__
  0%|          | 0/2513 [00:04<?, ?it/s]
    self.close()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/tqdm/std.py", line 1223, in close
  0%|          | 0/2513 [00:03<?, ?it/s]














































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 0/2513 [00:05<?, ?it/s]
    self._decr_instances(self)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/tqdm/std.py", line 546, in _decr_instances












































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A










































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A









































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A








































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A







































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































                                        [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A












































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:08<6:11:03,  8.86s/it]    for inst in cls._instances:
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/_weakrefset.py", line 60, in __iter__

    for itemref in self.data:
RuntimeError: Set changed size during iteration
[W 2023-08-14 15:36:27,072] Trial 29 failed with parameters: {'N': 8, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 24, in forward
    x = self.sublayer[1](x, lambda x: self.src_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 25, in <lambda>
    x, m, m, src_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 52, in forward
    x = x.transpose(1, 2).contiguous() \
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)
  0%|          | 0/2513 [00:05<?, ?it/s]
[W 2023-08-14 15:36:27,076] Trial 36 failed with parameters: {'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 79.15 GiB total capacity; 75.48 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 20, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positionwise_feedforward.py", line 15, in forward
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 79.15 GiB total capacity; 75.48 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,456] Trial 36 failed with value None.
[W 2023-08-14 15:36:27,178] Trial 50 failed with parameters: {'N': 10, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 79.15 GiB total capacity; 75.48 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 22, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 23, in <lambda>
    x, x, x, tgt_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 19, in attention
    p_attn = dropout(p_attn)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/dropout.py", line 54, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 807, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 79.15 GiB total capacity; 75.48 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,458] Trial 50 failed with value None.
[W 2023-08-14 15:36:27,223] Trial 23 failed with parameters: {'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.46 GiB already allocated; 11.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 22, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 23, in <lambda>
    x, x, x, tgt_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 16, in attention
    scores = scores.masked_fill(mask == 0, -1e9)
RuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.46 GiB already allocated; 11.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,458] Trial 23 failed with value None.
[W 2023-08-14 15:36:27,313] Trial 35 failed with parameters: {'N': 2, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/embeddings.py", line 13, in forward
    return self.lut(x) * math.sqrt(self.d_model)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 114, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1484, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,462] Trial 35 failed with value None.
[W 2023-08-14 15:36:27,360] Trial 44 failed with parameters: {'N': 6, 'd_model': 128, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 79.15 GiB total capacity; 75.48 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 18, in __call__
    loss.backward()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 42.00 MiB (GPU 0; 79.15 GiB total capacity; 75.48 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,465] Trial 44 failed with value None.
[W 2023-08-14 15:36:27,379] Trial 52 failed with parameters: {'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.50 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 18, in __call__
    loss.backward()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.50 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,466] Trial 52 failed with value None.
[W 2023-08-14 15:36:27,380] Trial 54 failed with parameters: {'N': 8, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 14, in attention
    / math.sqrt(d_k)
RuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,491] Trial 54 failed with value None.
[W 2023-08-14 15:36:27,422] Trial 38 failed with parameters: {'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 24, in forward
    x = self.sublayer[1](x, lambda x: self.src_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 25, in <lambda>
    x, m, m, src_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in forward
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in <listcomp>
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,506] Trial 38 failed with value None.
[W 2023-08-14 15:36:27,424] Trial 16 failed with parameters: {'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 18, in __call__
    loss.backward()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,522] Trial 16 failed with value None.
[W 2023-08-14 15:36:27,435] Trial 21 failed with parameters: {'N': 4, 'd_model': 512, 'd_ff': 1024, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 18, in __call__
    loss.backward()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,524] Trial 21 failed with value None.
[W 2023-08-14 15:36:27,453] Trial 40 failed with parameters: {'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.43 GiB already allocated; 21.25 MiB free; 78.30 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 18, in __call__
    loss.backward()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.43 GiB already allocated; 21.25 MiB free; 78.30 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,526] Trial 40 failed with value None.
[W 2023-08-14 15:36:27,296] Trial 45 failed with parameters: {'N': 8, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 204, in train
    model = self.get_model(opt, vocab, device, trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 55, in get_model
    model.to(device)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 425, in to
    return self._apply(convert)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 201, in _apply
    module._apply(fn)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 201, in _apply
    module._apply(fn)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 201, in _apply
    module._apply(fn)
  [Previous line repeated 2 more times]
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 223, in _apply
    param_applied = fn(param)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 423, in convert
    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,528] Trial 45 failed with value None.
[W 2023-08-14 15:36:27,330] Trial 28 failed with parameters: {'N': 2, 'd_model': 128, 'd_ff': 2048, 'h': 4, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 79.15 GiB total capacity; 75.44 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 20, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positionwise_feedforward.py", line 15, in forward
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 79.15 GiB total capacity; 75.44 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,529] Trial 28 failed with value None.
[W 2023-08-14 15:36:27,403] Trial 12 failed with parameters: {'N': 10, 'd_model': 256, 'd_ff': 1024, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.48 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 24, in forward
    x = self.sublayer[1](x, lambda x: self.src_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 25, in <lambda>
    x, m, m, src_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in forward
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in <listcomp>
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.48 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,529] Trial 12 failed with value None.
[W 2023-08-14 15:36:27,423] Trial 0 failed with parameters: {'N': 8, 'd_model': 256, 'd_ff': 2048, 'h': 4, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 79.15 GiB total capacity; 75.44 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 18, in __call__
    loss.backward()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 68.00 MiB (GPU 0; 79.15 GiB total capacity; 75.44 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,532] Trial 0 failed with value None.
[W 2023-08-14 15:36:27,436] Trial 29 failed with value None.
[W 2023-08-14 15:36:27,222] Trial 5 failed with parameters: {'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 34.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 27, in forward
    return self.sublayer[2](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positionwise_feedforward.py", line 15, in forward
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 34.00 MiB (GPU 0; 79.15 GiB total capacity; 75.47 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,534] Trial 5 failed with value None.
[W 2023-08-14 15:36:27,371] Trial 46 failed with parameters: {'N': 10, 'd_model': 64, 'd_ff': 1024, 'h': 8, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.51 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 24, in forward
    x = self.sublayer[1](x, lambda x: self.src_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 25, in <lambda>
    x, m, m, src_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 14, in attention
    / math.sqrt(d_k)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.51 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,534] Trial 46 failed with value None.
  0%|          | 0/2513 [00:05<?, ?it/s]
[W 2023-08-14 15:36:27,393] Trial 22 failed with parameters: {'N': 4, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 22, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/layer_norm.py", line 17, in forward
    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,537] Trial 22 failed with value None.
[W 2023-08-14 15:36:27,493] Trial 42 failed with parameters: {'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.41 GiB already allocated; 21.25 MiB free; 78.30 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 20, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positionwise_feedforward.py", line 15, in forward
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/dropout.py", line 54, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 807, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.41 GiB already allocated; 21.25 MiB free; 78.30 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,537] Trial 42 failed with value None.
[W 2023-08-14 15:36:27,433] Trial 47 failed with parameters: {'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 20, in attention
    return torch.matmul(p_attn, value), p_attn
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.53 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,537] Trial 47 failed with value None.
[W 2023-08-14 15:36:27,511] Trial 39 failed with parameters: {'N': 8, 'd_model': 512, 'd_ff': 2048, 'h': 4, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.42 GiB already allocated; 21.25 MiB free; 78.30 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 18, in __call__
    loss.backward()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.42 GiB already allocated; 21.25 MiB free; 78.30 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,538] Trial 39 failed with value None.
[W 2023-08-14 15:36:27,535] Trial 8 failed with parameters: {'N': 4, 'd_model': 512, 'd_ff': 256, 'h': 8, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.43 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 18, in __call__
    loss.backward()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.43 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,538] Trial 8 failed with value None.
[W 2023-08-14 15:36:27,568] Trial 11 failed with parameters: {'N': 10, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 79.15 GiB total capacity; 75.15 GiB already allocated; 43.25 MiB free; 78.28 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 18, in __call__
    loss.backward()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/tensor.py", line 195, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/autograd/__init__.py", line 99, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 79.15 GiB total capacity; 75.15 GiB already allocated; 43.25 MiB free; 78.28 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,569] Trial 11 failed with value None.
[W 2023-08-14 15:36:27,583] Trial 19 failed with parameters: {'N': 2, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 79.15 GiB total capacity; 75.22 GiB already allocated; 55.25 MiB free; 78.27 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 20, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positionwise_feedforward.py", line 15, in forward
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 79.15 GiB total capacity; 75.22 GiB already allocated; 55.25 MiB free; 78.27 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:27,584] Trial 19 failed with value None.
























  0%|          | 2/2513 [00:11<4:33:08,  6.53s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



  0%|          | 2/2513 [00:19<8:01:44, 11.51s/it] [A[A[A[A




  0%|          | 2/2513 [00:19<8:04:54, 11.59s/it] [A[A[A[A[A










  0%|          | 2/2513 [00:18<7:41:43, 11.03s/it][A[A[A[A[A[A[A[A[A[A[A




















  0%|          | 2/2513 [00:12<4:56:21,  7.08s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























  0%|          | 1/2513 [00:09<6:34:32,  9.42s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A












  0%|          | 1/2513 [00:19<13:43:32, 19.67s/it][A[A[A[A[A[A[A[A[A[A[A[A[A









  0%|          | 1/2513 [00:20<14:00:19, 20.07s/it][A[A[A[A[A[A[A[A[A[A









































  0%|          | 1/2513 [00:07<5:30:43,  7.90s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




































  0%|          | 1/2513 [00:08<6:05:47,  8.74s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































  0%|          | 1/2513 [00:09<6:17:29,  9.02s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



  0%|          | 3/2513 [00:20<5:56:16,  8.52s/it][A[A[A[A





























  0%|          | 1/2513 [00:10<7:35:38, 10.88s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A








































  0%|          | 1/2513 [00:08<5:56:29,  8.52s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
  0%|          | 1/2513 [00:20<14:32:34, 20.84s/it][A










  0%|          | 3/2513 [00:20<5:42:49,  8.19s/it][A[A[A[A[A[A[A[A[A[A[A




























  0%|          | 1/2513 [00:11<7:42:50, 11.06s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















  0%|          | 1/2513 [00:13<9:08:33, 13.10s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


































  0%|          | 1/2513 [00:09<6:28:09,  9.27s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




  0%|          | 3/2513 [00:21<6:02:58,  8.68s/it][A[A[A[A[A























  0%|          | 3/2513 [00:13<3:37:23,  5.20s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















  0%|          | 3/2513 [00:15<3:55:04,  5.62s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































  0%|          | 1/2513 [00:10<7:15:12, 10.39s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A










































  0%|          | 1/2513 [00:08<6:02:17,  8.65s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































  0%|          | 1/2513 [00:10<7:05:34, 10.16s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



































  0%|          | 1/2513 [00:10<7:00:32, 10.04s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





  0%|          | 1/2513 [00:21<15:06:29, 21.65s/it][A[A[A[A[A[A






































  0%|          | 1/2513 [00:09<6:45:33,  9.69s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A







































  0%|          | 1/2513 [00:09<6:50:49,  9.81s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






  0%|          | 1/2513 [00:21<15:17:03, 21.90s/it][A[A[A[A[A[A[A





































  0%|          | 1/2513 [00:10<7:04:30, 10.14s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A













  0%|          | 1/2513 [00:21<15:02:20, 21.55s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A















  0%|          | 1/2513 [00:21<14:49:14, 21.24s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[W 2023-08-14 15:36:31,303] Trial 37 failed with parameters: {'N': 8, 'd_model': 64, 'd_ff': 256, 'h': 8, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 17.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 24, in forward
    x = self.sublayer[1](x, lambda x: self.src_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 25, in <lambda>
    x, m, m, src_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in forward
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in <listcomp>
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 17.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,396] Trial 37 failed with value None.
[W 2023-08-14 15:36:31,373] Trial 6 failed with parameters: {'N': 8, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 17.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 22, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 23, in <lambda>
    x, x, x, tgt_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in forward
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in <listcomp>
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 17.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,434] Trial 6 failed with value None.
[W 2023-08-14 15:36:31,396] Trial 43 failed with parameters: {'N': 2, 'd_model': 256, 'd_ff': 1024, 'h': 4, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.92 GiB already allocated; 17.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 20, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positionwise_feedforward.py", line 15, in forward
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.92 GiB already allocated; 17.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,468] Trial 43 failed with value None.
[W 2023-08-14 15:36:31,415] Trial 14 failed with parameters: {'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 52, in forward
    x = x.transpose(1, 2).contiguous() \
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,532] Trial 14 failed with value None.

















  0%|          | 1/2513 [00:19<13:23:36, 19.19s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















  0%|          | 1/2513 [00:18<13:00:53, 18.65s/it][A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A








  0%|          | 1/2513 [00:22<15:56:06, 22.84s/it][A[A[A[A[A[A[A[A[A[W 2023-08-14 15:36:31,454] Trial 10 failed with parameters: {'N': 6, 'd_model': 128, 'd_ff': 512, 'h': 16, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positional_encoding.py", line 27, in forward
    return self.dropout(x)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/dropout.py", line 54, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 807, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 15.25 MiB free; 78.31 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,895] Trial 10 failed with value None.
[W 2023-08-14 15:36:31,423] Trial 26 failed with parameters: {'N': 4, 'd_model': 64, 'd_ff': 1024, 'h': 16, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 13, in attention
    scores = torch.matmul(query, key.transpose(-2, -1)) \
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,896] Trial 26 failed with value None.
[W 2023-08-14 15:36:31,572] Trial 48 failed with parameters: {'N': 2, 'd_model': 256, 'd_ff': 512, 'h': 16, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 13, in attention
    scores = torch.matmul(query, key.transpose(-2, -1)) \
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,898] Trial 48 failed with value None.
[W 2023-08-14 15:36:31,571] Trial 15 failed with parameters: {'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 7.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 24, in forward
    x = self.sublayer[1](x, lambda x: self.src_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 25, in <lambda>
    x, m, m, src_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 13, in attention
    scores = torch.matmul(query, key.transpose(-2, -1)) \
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 7.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,899] Trial 15 failed with value None.
[W 2023-08-14 15:36:31,572] Trial 30 failed with parameters: {'N': 8, 'd_model': 256, 'd_ff': 256, 'h': 4, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 79.15 GiB total capacity; 75.89 GiB already allocated; 39.25 MiB free; 78.29 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 16, in attention
    scores = scores.masked_fill(mask == 0, -1e9)
RuntimeError: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 79.15 GiB total capacity; 75.89 GiB already allocated; 39.25 MiB free; 78.29 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,900] Trial 30 failed with value None.
[W 2023-08-14 15:36:31,754] Trial 17 failed with parameters: {'N': 2, 'd_model': 128, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 20, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/layer_norm.py", line 17, in forward
    return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,906] Trial 17 failed with value None.
[W 2023-08-14 15:36:31,758] Trial 18 failed with parameters: {'N': 4, 'd_model': 512, 'd_ff': 512, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/embeddings.py", line 13, in forward
    return self.lut(x) * math.sqrt(self.d_model)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 114, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1484, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,835] Trial 51 failed with parameters: {'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 20, in attention
    return torch.matmul(p_attn, value), p_attn
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,917] Trial 51 failed with value None.
[W 2023-08-14 15:36:31,843] Trial 13 failed with parameters: {'N': 2, 'd_model': 128, 'd_ff': 1024, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 20, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positionwise_feedforward.py", line 15, in forward
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 38.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,960] Trial 13 failed with value None.
[W 2023-08-14 15:36:31,531] Trial 7 failed with parameters: {'N': 4, 'd_model': 256, 'd_ff': 2048, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 79.15 GiB total capacity; 75.88 GiB already allocated; 51.25 MiB free; 78.28 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 20, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positionwise_feedforward.py", line 15, in forward
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/dropout.py", line 54, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 807, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 80.00 MiB (GPU 0; 79.15 GiB total capacity; 75.88 GiB already allocated; 51.25 MiB free; 78.28 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,967] Trial 7 failed with value None.
[W 2023-08-14 15:36:31,679] Trial 34 failed with parameters: {'N': 4, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 7.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 24, in forward
    x = self.sublayer[1](x, lambda x: self.src_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 25, in <lambda>
    x, m, m, src_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 13, in attention
    scores = torch.matmul(query, key.transpose(-2, -1)) \
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 7.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,967] Trial 34 failed with value None.
[W 2023-08-14 15:36:31,687] Trial 32 failed with parameters: {'N': 10, 'd_model': 128, 'd_ff': 256, 'h': 4, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 7.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/embeddings.py", line 13, in forward
    return self.lut(x) * math.sqrt(self.d_model)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 114, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1484, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 7.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,968] Trial 32 failed with value None.
[W 2023-08-14 15:36:31,800] Trial 33 failed with parameters: {'N': 6, 'd_model': 64, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 79.15 GiB total capacity; 75.89 GiB already allocated; 39.25 MiB free; 78.29 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 24, in forward
    x = self.sublayer[1](x, lambda x: self.src_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 25, in <lambda>
    x, m, m, src_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 16, in attention
    scores = scores.masked_fill(mask == 0, -1e9)
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 79.15 GiB total capacity; 75.89 GiB already allocated; 39.25 MiB free; 78.29 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,968] Trial 33 failed with value None.
[W 2023-08-14 15:36:31,777] Trial 41 failed with parameters: {'N': 8, 'd_model': 256, 'd_ff': 512, 'h': 4, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 31, in forward
    tgt, tgt_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 37, in decode
    return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder.py", line 21, in forward
    x = layer(x, memory, src_mask, tgt_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 22, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/decoder_layer.py", line 23, in <lambda>
    x, x, x, tgt_mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 19, in attention
    p_attn = dropout(p_attn)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/dropout.py", line 54, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 807, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.95 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,969] Trial 41 failed with value None.
[W 2023-08-14 15:36:31,907] Trial 18 failed with value None.
[W 2023-08-14 15:36:31,873] Trial 4 failed with parameters: {'N': 4, 'd_model': 256, 'd_ff': 256, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/embeddings.py", line 13, in forward
    return self.lut(x) * math.sqrt(self.d_model)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,970] Trial 4 failed with value None.
[W 2023-08-14 15:36:31,660] Trial 24 failed with parameters: {'N': 6, 'd_model': 128, 'd_ff': 2048, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 14, in attention
    / math.sqrt(d_k)
RuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,982] Trial 24 failed with value None.
[W 2023-08-14 15:36:31,975] Trial 49 failed with parameters: {'N': 6, 'd_model': 512, 'd_ff': 2048, 'h': 8, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.94 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:31,934] Trial 27 failed with parameters: {'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 16, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 104, in train_epoch
    loss = loss_compute(out, trg_y, ntokens)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/simpleloss_compute.py", line 19, in __call__
    self.opt.step()
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/noam_opt.py", line 20, in step
    self.optimizer.step()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/optim/adam.py", line 103, in step
    denom = (exp_avg_sq.sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,071] Trial 27 failed with value None.
[W 2023-08-14 15:36:31,987] Trial 49 failed with value None.
[W 2023-08-14 15:36:31,948] Trial 55 failed with parameters: {'N': 10, 'd_model': 64, 'd_ff': 256, 'h': 4, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in forward
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 45, in <listcomp>
    for l, x in zip(self.linears, (query, key, value))]
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,073] Trial 55 failed with value None.
[W 2023-08-14 15:36:32,164] Trial 2 failed with parameters: {'N': 6, 'd_model': 128, 'd_ff': 1024, 'h': 16, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.96 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/embeddings.py", line 13, in forward
    return self.lut(x) * math.sqrt(self.d_model)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 114, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1484, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.96 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,187] Trial 53 failed with parameters: {'N': 6, 'd_model': 256, 'd_ff': 512, 'h': 8, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/embeddings.py", line 13, in forward
    return self.lut(x) * math.sqrt(self.d_model)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.93 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,242] Trial 53 failed with value None.
[W 2023-08-14 15:36:32,240] Trial 2 failed with value None.
[W 2023-08-14 15:36:32,228] Trial 1 failed with parameters: {'N': 10, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 79.15 GiB total capacity; 75.96 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 13, in attention
    scores = torch.matmul(query, key.transpose(-2, -1)) \
RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 79.15 GiB total capacity; 75.96 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,243] Trial 1 failed with value None.
[W 2023-08-14 15:36:32,402] Trial 9 failed with parameters: {'N': 6, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.98 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 13, in attention
    scores = torch.matmul(query, key.transpose(-2, -1)) \
RuntimeError: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 79.15 GiB total capacity; 75.98 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,403] Trial 9 failed with value None.
[W 2023-08-14 15:36:32,414] Trial 20 failed with parameters: {'N': 2, 'd_model': 512, 'd_ff': 256, 'h': 4, 'dropout': 0.1} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.97 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/embeddings.py", line 13, in forward
    return self.lut(x) * math.sqrt(self.d_model)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/sparse.py", line 114, in forward
    self.norm_type, self.scale_grad_by_freq, self.sparse)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1484, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 79.15 GiB total capacity; 75.97 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,415] Trial 20 failed with value None.
[W 2023-08-14 15:36:32,461] Trial 31 failed with parameters: {'N': 10, 'd_model': 64, 'd_ff': 2048, 'h': 4, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.15 GiB total capacity; 75.98 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/container.py", line 100, in forward
    input = module(input)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positional_encoding.py", line 27, in forward
    return self.dropout(x)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/dropout.py", line 54, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 807, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.15 GiB total capacity; 75.98 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,463] Trial 31 failed with value None.
[W 2023-08-14 15:36:32,461] Trial 3 failed with parameters: {'N': 6, 'd_model': 64, 'd_ff': 512, 'h': 4, 'dropout': 0.5} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 79.15 GiB total capacity; 76.01 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in forward
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 19, in <lambda>
    x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 49, in forward
    dropout=self.dropout)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/multi_headed_attention.py", line 13, in attention
    scores = torch.matmul(query, key.transpose(-2, -1)) \
RuntimeError: CUDA out of memory. Tried to allocate 12.00 MiB (GPU 0; 79.15 GiB total capacity; 76.01 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,464] Trial 3 failed with value None.
[W 2023-08-14 15:36:32,475] Trial 25 failed with parameters: {'N': 2, 'd_model': 64, 'd_ff': 2048, 'h': 16, 'dropout': 0.30000000000000004} because of the following error: RuntimeError('CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.15 GiB total capacity; 76.00 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)').
Traceback (most recent call last):
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 100, in train_epoch
    trg_mask = trg_mask.to(device)
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 79.15 GiB total capacity; 76.00 GiB already allocated; 3.25 MiB free; 78.32 GiB reserved in total by PyTorch)
[W 2023-08-14 15:36:32,476] Trial 25 failed with value None.
Traceback (most recent call last):
  File "train.py", line 30, in <module>
    study.optimize(objective,n_jobs = -1)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/study.py", line 452, in optimize
    show_progress_bar=show_progress_bar,
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 103, in _optimize
    f.result()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/concurrent/futures/_base.py", line 428, in result
    return self.__get_result()
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/concurrent/futures/thread.py", line 57, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 163, in _optimize_sequential
    frozen_trial = _run_trial(study, func, catch)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 251, in _run_trial
    raise func_err
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/optuna/study/_optimize.py", line 200, in _run_trial
    value_or_values = func(trial)
  File "train.py", line 22, in objective
    loss_epoch_train, loss_epoch_validation, accuracy = trainer.train(opt=opt,trial=trial)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 221, in train
    optim), device)
  File "/vols/opig/users/raja/deep-molecular-optimization/trainer/transformer_trainer.py", line 103, in train_epoch
    out = model.forward(src, trg, src_mask, trg_mask)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 30, in forward
    return self.decode(self.encode(src, src_mask), src_mask,
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/model.py", line 34, in encode
    return self.encoder(self.src_embed(src), src_mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder.py", line 19, in forward
    x = layer(x, mask)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/encoder_layer.py", line 20, in forward
    return self.sublayer[1](x, self.feed_forward)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/encode_decode/sublayer_connection.py", line 18, in forward
    return x + self.dropout(sublayer(self.norm(x)))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/deep-molecular-optimization/models/transformer/module/positionwise_feedforward.py", line 15, in forward
    return self.w_2(self.dropout(F.relu(self.w_1(x))))
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/module.py", line 532, in __call__
    result = self.forward(*input, **kwargs)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/modules/linear.py", line 87, in forward
    return F.linear(input, self.weight, self.bias)
  File "/vols/opig/users/raja/miniconda3/envs/molopt/lib/python3.7/site-packages/torch/nn/functional.py", line 1372, in linear
    output = input.matmul(weight.t())
RuntimeError: CUDA out of memory. Tried to allocate 18.00 MiB (GPU 0; 79.15 GiB total capacity; 75.48 GiB already allocated; 5.25 MiB free; 78.32 GiB reserved in total by PyTorch)











































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A









































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





                                                   [A[A[A[A[A[A














                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A











                                                   [A[A[A[A[A[A[A[A[A[A[A[A







                                                   [A[A[A[A[A[A[A[A








                                                   [A[A[A[A[A[A[A[A[A
                                                   [A
























                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















                                                   [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















                                                   [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















                                                   [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A








































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A







































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A












                                                   [A[A[A[A[A[A[A[A[A[A[A[A[A


                                                  [A[A[A










                                                  [A[A[A[A[A[A[A[A[A[A[A






                                                   [A[A[A[A[A[A[A



                                                  [A[A[A[A













                                                   [A[A[A[A[A[A[A[A[A[A[A[A[A[A




                                                  [A[A[A[A[A









                                                   [A[A[A[A[A[A[A[A[A[A

                                                  [A[A




















                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























                                                  [A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:23<16:43:59, 23.98s/it]










































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A








































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A







































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:12<8:51:18, 12.69s/it]









































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A







































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:12<8:22:59, 12.01s/it]








































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A













[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A














[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:23<16:06:47, 23.09s/it]







































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




[A[A[A[A[A












[A[A[A[A[A[A[A[A[A[A[A[A[A










[A[A[A[A[A[A[A[A[A[A[A






[A[A[A[A[A[A[A







[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A














[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A













[A[A[A[A[A[A[A[A[A[A[A[A[A[A




































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A











[A[A[A[A[A[A[A[A[A[A[A[A









[A[A[A[A[A[A[A[A[A[A





[A[A[A[A[A[A








[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 3/2513 [00:23<5:33:53,  7.98s/it]






































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:16<11:11:20, 16.04s/it]





































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



[A[A[A[A











[A[A[A[A[A[A[A[A[A[A[A[A









[A[A[A[A[A[A[A[A[A[A





[A[A[A[A[A[A






[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A














[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A













[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A












[A[A[A[A[A[A[A[A[A[A[A[A[A


































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A










[A[A[A[A[A[A[A[A[A[A[A








[A[A[A[A[A[A[A[A[A




[A[A[A[A[A







[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 3/2513 [00:23<5:33:54,  7.98s/it]




































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 3/2513 [00:17<4:05:29,  5.87s/it]



































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:14<10:10:30, 14.58s/it]


































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A














[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:19<13:34:51, 19.46s/it]

































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:12<8:33:19, 12.26s/it]
































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:12<8:33:27, 12.26s/it]































[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:11<7:49:04, 11.20s/it]






























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 3/2513 [00:15<3:41:58,  5.31s/it]





























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:12<8:33:11, 12.26s/it]




























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:17<12:22:03, 17.72s/it]



























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:15<10:48:08, 15.48s/it]


























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A














[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:19<13:31:43, 19.39s/it]

























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:11<8:16:45, 11.87s/it]
























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:11<8:16:40, 11.86s/it]























[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:11<8:09:34, 11.69s/it]






















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A














[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A












[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A













[A[A[A[A[A[A[A[A[A[A[A[A[A[A















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:22<15:50:16, 22.70s/it]





















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:12<9:01:43, 12.94s/it]




















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A













[A[A[A[A[A[A[A[A[A[A[A[A[A[A















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A
















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A












[A[A[A[A[A[A[A[A[A[A[A[A[A














[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:20<13:59:06, 20.04s/it]



















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:11<8:16:52, 11.87s/it]


















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:12<8:41:13, 12.45s/it]

















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A










[A[A[A[A[A[A[A[A[A[A[A












[A[A[A[A[A[A[A[A[A[A[A[A[A














[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A











[A[A[A[A[A[A[A[A[A[A[A[A













[A[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:23<16:09:24, 23.15s/it]
















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A











[A[A[A[A[A[A[A[A[A[A[A[A













[A[A[A[A[A[A[A[A[A[A[A[A[A[A














[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A












[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:16<11:17:26, 16.18s/it]















[A[A[A[A[A[A[A[A[A[A[A[A[A[A[A


[A[A[A









[A[A[A[A[A[A[A[A[A[A








[A[A[A[A[A[A[A[A[A




[A[A[A[A[A





[A[A[A[A[A[A










[A[A[A[A[A[A[A[A[A[A[A












[A[A[A[A[A[A[A[A[A[A[A[A[A













[A[A[A[A[A[A[A[A[A[A[A[A[A[A

[A[A







[A[A[A[A[A[A[A[A



[A[A[A[A






[A[A[A[A[A[A[A











[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 2/2513 [00:23<8:21:50, 11.99s/it]














[A[A[A[A[A[A[A[A[A[A[A[A[A[A








[A[A[A[A[A[A[A[A[A







[A[A[A[A[A[A[A[A









[A[A[A[A[A[A[A[A[A[A











[A[A[A[A[A[A[A[A[A[A[A[A












[A[A[A[A[A[A[A[A[A[A[A[A[A






[A[A[A[A[A[A[A










[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:23<16:21:18, 23.44s/it]













[A[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:14<9:49:01, 14.07s/it]












[A[A[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:14<9:52:30, 14.15s/it]











[A[A[A[A[A[A[A[A[A[A[A







[A[A[A[A[A[A[A[A






[A[A[A[A[A[A[A








[A[A[A[A[A[A[A[A[A





[A[A[A[A[A[A









[A[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:23<16:26:49, 23.57s/it]










[A[A[A[A[A[A[A[A[A[A






[A[A[A[A[A[A[A





[A[A[A[A[A[A







[A[A[A[A[A[A[A[A




[A[A[A[A[A








[A[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:23<16:37:59, 23.84s/it]









[A[A[A[A[A[A[A[A[A





[A[A[A[A[A[A






[A[A[A[A[A[A[A







[A[A[A[A[A[A[A[A  0%|          | 1/2513 [00:23<16:09:26, 23.16s/it]








[A[A[A[A[A[A[A[A





[A[A[A[A[A[A






[A[A[A[A[A[A[A  0%|          | 2/2513 [00:23<8:03:15, 11.55s/it]







[A[A[A[A[A[A[A




[A[A[A[A[A



[A[A[A[A


[A[A[A





[A[A[A[A[A[A  0%|          | 1/2513 [00:23<16:38:19, 23.85s/it]
  0%|          | 1/2513 [00:10<7:38:15, 10.95s/it]




[A[A[A[A
[A


[A[A[A

[A[A




[A[A[A[A[A  0%|          | 1/2513 [00:23<16:44:07, 23.98s/it]




[A[A[A[A  0%|          | 1/2513 [00:15<11:06:25, 15.92s/it]


[A[A
[A


[A[A[A  0%|          | 2/2513 [00:23<8:21:08, 11.97s/it]

[A

[A[A  0%|          | 1/2513 [00:23<16:38:03, 23.84s/it]

[A  0%|          | 3/2513 [00:23<5:22:54,  7.72s/it]
  0%|          | 1/2513 [00:15<11:01:38, 15.80s/it]
